models:
  - name: qwen3-embedding-8b
    modelname: qwen/qwen3-embedding-8b
    provider:
      type: vllm
      subtype: mxfp4
    command: >
      vllm serve qwen/qwen3-embedding-8b
        --host 0.0.0.0
        --port 8000
        --served-model-name qwen3-embedding-8b
        --kv-cache-dtype fp8
        --tensor-parallel-size 1
        --enable-prefix-caching
        --load-format fastsafetensors
        --gpu_memory_utilization 0.15
        --enable-prompt-embeds
        --max-model-len 4096
        --max-num-seqs 256

  - name: qwen3-reranker-8b
    modelname: qwen/qwen3-reranker-8b
    provider:
      type: vllm
      subtype: mxfp4
    command: >
      vllm serve qwen/qwen3-reranker-8b
        --host 0.0.0.0
        --port 8000
        --served-model-name qwen3-reranker-8b
        --kv-cache-dtype fp8
        --tensor-parallel-size 1
        --enable-prefix-caching
        --load-format fastsafetensors
        --gpu_memory_utilization 0.4
        --max-model-len 4096
        --max-num-seqs 256

  - name: gpt-oss-20b-4096
    modelname: openai/gpt-oss-20b
    provider:
      type: vllm
      subtype: mxfp4
    command: >
      vllm serve openai/gpt-oss-20b
        --host 0.0.0.0
        --port 8000
        --served-model-name gpt-oss-20b-4096
        --quantization mxfp4
        --mxfp4-backend CUTLASS
        --mxfp4-layers moe,qkv,o,lm_head
        --attention-backend FLASHINFER
        --kv-cache-dtype fp8
        --tensor-parallel-size 1
        --max-model-len 4096
        --max-num-seqs 512
        --max-num-batched-tokens 32768
        --enable-prefix-caching
        --load-format fastsafetensors
        --gpu_memory_utilization 0.4

  - name: gpt-oss-20b-32768
    modelname: openai/gpt-oss-20b
    provider:
      type: vllm
      subtype: mxfp4
    command: >
      vllm serve openai/gpt-oss-20b
        --host 0.0.0.0
        --port 8000
        --served-model-name gpt-oss-20b-32768
        --quantization mxfp4
        --mxfp4-backend CUTLASS
        --mxfp4-layers moe,qkv,o,lm_head
        --attention-backend FLASHINFER
        --kv-cache-dtype fp8
        --tensor-parallel-size 1
        --max-model-len 32768
        --max-num-seqs 256
        --max-num-batched-tokens 32768
        --enable-prefix-caching
        --load-format fastsafetensors
        --gpu_memory_utilization 0.7

  - name: gpt-oss-20b-131072
    modelname: openai/gpt-oss-20b
    provider:
      type: vllm
      subtype: mxfp4
    command: >
      vllm serve openai/gpt-oss-20b
        --host 0.0.0.0
        --port 8000
        --served-model-name gpt-oss-20b-131072
        --quantization mxfp4
        --mxfp4-backend CUTLASS
        --mxfp4-layers moe,qkv,o,lm_head
        --attention-backend FLASHINFER
        --kv-cache-dtype fp8
        --tensor-parallel-size 1
        --max-model-len 131072
        --max-num-seqs 32
        --max-num-batched-tokens 32768
        --enable-prefix-caching
        --load-format fastsafetensors
        --gpu_memory_utilization 0.1

  - name: gpt-oss-120b-131072
    modelname: openai/gpt-oss-120b
    provider:
      type: vllm
      subtype: mxfp4
    command: >
      vllm serve openai/gpt-oss-120b
        --host 0.0.0.0
        --port 8000
        --served-model-name gpt-oss-120b-131072
        --quantization mxfp4
        --mxfp4-backend CUTLASS
        --mxfp4-layers moe,qkv,o,lm_head
        --attention-backend FLASHINFER
        --kv-cache-dtype fp8
        --tensor-parallel-size 1
        --max-model-len 131072
        --max-num-seqs 3
        --max-num-batched-tokens 65536
        --enable-prefix-caching
        --load-format fastsafetensors
        --gpu_memory_utilization 0.7
        --tool-call-parser openai
        --enable-auto-tool-choice

  - name: nemotron-3
    modelname: nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4
    provider:
      type: vllm
      subtype: 0141t4
    env:
      - VLLM_USE_FLASHINFER_MOE_FP4=1
      - VLLM_FLASHINFER_MOE_BACKEND=throughput
    command: >
      vllm serve nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4
        --host 0.0.0.0
        --port 8000
        --served-model-name nemotron-3
        --kv-cache-dtype fp8
        --tensor-parallel-size 1
        --enable-prefix-caching
        --load-format fastsafetensors
        --gpu_memory_utilization 0.8
        --trust_remote_code

  - name: glm-4.7-flash
    modelname: zai-org/GLM-4.7-Flash
    provider:
      type: vllm
      subtype: default
    command: >
      vllm serve zai-org/GLM-4.7-Flash
        --host 0.0.0.0
        --port 8000
        --served-model-name glm-4.7-flash
        --tensor-parallel-size 1
        --enable-prefix-caching
        --load-format fastsafetensors
        --gpu_memory_utilization 0.8
        --tool-call-parser glm47
        --reasoning-parser glm45
        --enable-auto-tool-choice
        --max-num-batched-tokens 4096
        --max-model-len 202752

  - name: step-3.5-flash-int4
    modelname: stepfun-ai/Step-3.5-Flash-Int4
    provider:
      type: llamacpp
      subtype: step35
    concurrencyLimit: 4
    env:
      - LLAMA_ARG_THREADS=8
      - LLAMA_ARG_CTX_SIZE=131072
      - LLAMA_ARG_BATCH=66536
      - LLAMA_ARG_UBATCH=66536
    command: ["--metrics", "-m", "/root/.cache/huggingface/hub/models--stepfun-ai--Step-3.5-Flash-Int4/snapshots/8f4ece312b61a4e73f99291a949bdf58dbb7ad1f/step3p5_flash_Q4_K_S.gguf","--temp","1.0"]

  - name: qwen3-coder-next-fp8
    modelname: Qwen/Qwen3-Coder-Next-FP8
    provider:
      type: vllm
      subtype: default
    command: >
      vllm serve Qwen/Qwen3-Coder-Next-FP8
        --host 0.0.0.0
        --port 8000
        --served-model-name qwen3-coder-next-fp8
        --tensor-parallel-size 1
        --enable-prefix-caching
        --load-format fastsafetensors
        --gpu_memory_utilization 0.8
        --tool-call-parser qwen3_coder
        --enable-auto-tool-choice
        --max-num-batched-tokens 4096
        --max-model-len 202752

  - name: qwen3-coder-next-mxfp4
    modelname: unsloth/Qwen3-Coder-Next-GGUF
    provider:
      type: llamacpp
      subtype: default
    env:
      - LLAMA_ARG_BATCH=4096
      - LLAMA_ARG_UBATCH=4096
    command: ["--metrics", "-m", "/root/.cache/huggingface/hub/models--unsloth--Qwen3-Coder-Next-GGUF/snapshots/37d32cdfb01580f72e8f73f35f8c18e7e40b28ae/Qwen3-Coder-Next-MXFP4_MOE.gguf","--temp","1.0"]

  - name: minimax-m2.5-gguf-q3
    modelname: ox-ox/MiniMax-M2.5-GGUF
    provider:
      type: llamacpp
      subtype: default
    command: ["--metrics", "-m", "/root/.cache/huggingface/hub/models--ox-ox--MiniMax-M2.5-GGUF/snapshots/076c3e199a449df2ec01b6d52a762ee56fb9d702/minimax-m2.5-Q3_K_L.gguf"]

  - name: nemotron-3-gguf-bf16
    modelname: unsloth/Nemotron-3-Nano-30B-A3B-GGUF:BF16
    provider:
      type: llamacpp
      subtype: default
    env:
      - LLAMA_ARG_BATCH=4096
      - LLAMA_ARG_UBATCH=4096
    command: ["--metrics", "-hf", "unsloth/Nemotron-3-Nano-30B-A3B-GGUF:BF16","--temp","1.0"]

  - name: glm-4.6v-flash-gguf-bf16
    modelname: unsloth/GLM-4.6V-Flash-GGUF:BF16
    provider:
      type: llamacpp
      subtype: default
    env:
      - LLAMA_ARG_BATCH=4096
      - LLAMA_ARG_UBATCH=4096
    command: ["--metrics", "-hf", "unsloth/GLM-4.6V-Flash-GGUF:BF16","--temp","0.8"]

providers:
  - type: llamacpp
    subtype: base
    parameters:
      env:
        NVIDIA_VISIBLE_DEVICES: all
        NVIDIA_DRIVER_CAPABILITIES: compute,utility
        GGML_CUDA_ENABLE_UNIFIED_MEMORY: "1"
        LLAMA_ARG_MMAP: "0"
        LLAMA_ARG_HOST: 0.0.0.0
        LLAMA_ARG_WEBUI: "0"
        LLAMA_ARG_PORT: 8000
        LLAMA_ARG_THREADS: 20
        LLAMA_ARG_FLASH_ATTN: on
        LLAMA_ARG_N_GPU_LAYERS: all
        HF_TOKEN: ${HF_TOKEN}
      volumes:
        - ${HF_LOC}:/root/.cache/huggingface
        - ${LLAMACPP_LOC}:/root/.cache/llama.cpp
      restart: no
      networks:
        - llama-net
      deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                count: all
                capabilities: [gpu]
      ipc: host
      ulimits:
        memlock: -1
        stack: 67108864
      ttl: 600
      concurrencyLimit: 4096

  - type: llamacpp
    subtype: default
    inheritsfrom: base
    parameters:
      build:
        dockerfile: Dockerfile.llamacpp
        context: .
        image: llama-cpp:arm64cuda

  - type: llamacpp
    subtype: step35
    inheritsfrom: default
    parameters:
      build:
        dockerfile: Dockerfile.llamacppstep35
        context: .
        image: llama-cpp-step35:arm64cuda

  - type: vllm
    subtype: base
    parameters:
      env:
        FLASHINFER_LOGLEVEL: "0"
        FLASHINFER_JIT_VERBOSE: "0"
        FLASHINFER_NVCC_THREADS: "4"
        VLLM_NO_USAGE_STATS: "1"
        DO_NOT_TRACK: "1"
        TIKTOKEN_ENCODINGS_BASE: /workspace/tiktoken_encodings
      volumes:
        - ${HF_LOC}:/root/.cache/huggingface
        - ./.cache/flashinfer:/root/.cache/flashinfer
        - ./.cache/vllm:/root/.cache/vllm
        - ./.cache/ccache:/root/.ccache
      restart: no
      networks:
        - llama-net
      deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                count: all
                capabilities: [gpu]
      ipc: host
      ulimits:
        memlock: -1
        stack: 67108864
      ttl: 600
      concurrencyLimit: 4096

  - type: vllm
    subtype: default
    inheritsfrom: base
    parameters:
      image:
        image: scitrera/dgx-spark-vllm:0.16.0-t5
      command:
        - --host 0.0.0.0
        - --port 8000
        - --served-model-name
        - $(modelname)
        - --tensor-parallel-size 1
        - --enable-prefix-caching
        - --load-format fastsafetensors

  - type: vllm
    subtype: mxfp4
    inheritsfrom: default
    parameters:
      image:
        build:
          context: .
          dockerfile: Dockerfile.vllm
          args:
            BUILD_JOBS: 16
        image: vllm-mxfp4-spark:latest
      command:
        - --host 0.0.0.0
        - --port 8000
        - --served-model-name
        - $(modelname)
        - --quantization mxfp4
        - --mxfp4-backend CUTLASS
        - --mxfp4-layers moe,qkv,o,lm_head
        - --attention-backend FLASHINFER
        - --kv-cache-dtype fp8
        - --tensor-parallel-size 1
        - --enable-prefix-caching
        - --load-format fastsafetensors

  - type: vllm
    subtype: 0141t4
    inheritsfrom: default
    parameters:
      image:
        image: scitrera/dgx-spark-vllm:0.14.1-t4
      command:
        - --host 0.0.0.0
        - --port 8000
        - --served-model-name
        - $(modelname)
        - --kv-cache-dtype fp8
        - --tensor-parallel-size 1
        - --enable-prefix-caching
        - --load-format fastsafetensors


