services:
  qwen3-embedding-8b:
    container_name: vllm-qwen3-embedding-8b
    environment:
      - FLASHINFER_LOGLEVEL=0
      - FLASHINFER_JIT_VERBOSE=0
      - FLASHINFER_NVCC_THREADS=4
      - VLLM_NO_USAGE_STATS=1
      - DO_NOT_TRACK=1
      - TIKTOKEN_ENCODINGS_BASE=/workspace/tiktoken_encodings
    build:
      context: .
      dockerfile: Dockerfile.vllm
      args:
        BUILD_JOBS: 16
    image: vllm-mxfp4-spark:latest
    volumes:
      - ${HF_LOC}:/root/.cache/huggingface
      - ./.cache/flashinfer:/root/.cache/flashinfer
      - ./.cache/vllm:/root/.cache/vllm
      - ./.cache/ccache:/root/.ccache
    restart: no
    networks:
      - llama-net
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all
            capabilities:
            - gpu
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    healthcheck:
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8000/health
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s
    command: >
      vllm serve qwen/qwen3-embedding-8b
        --host 0.0.0.0
        --port 8000
        --served-model-name
        qwen3-embedding-8b
        --tensor-parallel-size 1
        --enable-prefix-caching
        --load-format fastsafetensors
        --kv-cache-dtype fp8
        --gpu_memory_utilization 0.15
        --enable-prompt-embeds
        --max-model-len 4096
        --max-num-seqs 256

  qwen3-reranker-8b:
    container_name: vllm-qwen3-reranker-8b
    environment:
      - FLASHINFER_LOGLEVEL=0
      - FLASHINFER_JIT_VERBOSE=0
      - FLASHINFER_NVCC_THREADS=4
      - VLLM_NO_USAGE_STATS=1
      - DO_NOT_TRACK=1
      - TIKTOKEN_ENCODINGS_BASE=/workspace/tiktoken_encodings
    build:
      context: .
      dockerfile: Dockerfile.vllm
      args:
        BUILD_JOBS: 16
    image: vllm-mxfp4-spark:latest
    volumes:
      - ${HF_LOC}:/root/.cache/huggingface
      - ./.cache/flashinfer:/root/.cache/flashinfer
      - ./.cache/vllm:/root/.cache/vllm
      - ./.cache/ccache:/root/.ccache
    restart: no
    networks:
      - llama-net
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all
            capabilities:
            - gpu
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    healthcheck:
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8000/health
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s
    command: >
      vllm serve qwen/qwen3-reranker-8b
        --host 0.0.0.0
        --port 8000
        --served-model-name
        qwen3-reranker-8b
        --tensor-parallel-size 1
        --enable-prefix-caching
        --load-format fastsafetensors
        --kv-cache-dtype fp8
        --gpu_memory_utilization 0.4
        --max-model-len 4096
        --max-num-seqs 256

  gpt-oss-20b-4096:
    container_name: gpt-oss-20b-4096
    environment:
      - FLASHINFER_LOGLEVEL=0
      - FLASHINFER_JIT_VERBOSE=0
      - FLASHINFER_NVCC_THREADS=4
      - VLLM_NO_USAGE_STATS=1
      - DO_NOT_TRACK=1
      - TIKTOKEN_ENCODINGS_BASE=/workspace/tiktoken_encodings
    build:
      context: .
      dockerfile: Dockerfile.vllm
      args:
        BUILD_JOBS: 16
    image: vllm-mxfp4-spark:latest
    volumes:
      - ${HF_LOC}:/root/.cache/huggingface
      - ./.cache/flashinfer:/root/.cache/flashinfer
      - ./.cache/vllm:/root/.cache/vllm
      - ./.cache/ccache:/root/.ccache
    restart: no
    networks:
      - llama-net
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all
            capabilities:
            - gpu
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    healthcheck:
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8000/health
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s
    command: >
      vllm serve openai/gpt-oss-20b
        --host 0.0.0.0
        --port 8000
        --served-model-name
        gpt-oss-20b-4096
        --quantization mxfp4
        --mxfp4-backend CUTLASS
        --mxfp4-layers moe,qkv,o,lm_head
        --attention-backend FLASHINFER
        --kv-cache-dtype fp8
        --tensor-parallel-size 1
        --enable-prefix-caching
        --load-format fastsafetensors
        --max-model-len 4096
        --max-num-seqs 512
        --max-num-batched-tokens 32768
        --gpu_memory_utilization 0.4

  gpt-oss-20b-32768:
    container_name: gpt-oss-20b-32768
    environment:
      - FLASHINFER_LOGLEVEL=0
      - FLASHINFER_JIT_VERBOSE=0
      - FLASHINFER_NVCC_THREADS=4
      - VLLM_NO_USAGE_STATS=1
      - DO_NOT_TRACK=1
      - TIKTOKEN_ENCODINGS_BASE=/workspace/tiktoken_encodings
    build:
      context: .
      dockerfile: Dockerfile.vllm
      args:
        BUILD_JOBS: 16
    image: vllm-mxfp4-spark:latest
    volumes:
      - ${HF_LOC}:/root/.cache/huggingface
      - ./.cache/flashinfer:/root/.cache/flashinfer
      - ./.cache/vllm:/root/.cache/vllm
      - ./.cache/ccache:/root/.ccache
    restart: no
    networks:
      - llama-net
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all
            capabilities:
            - gpu
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    healthcheck:
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8000/health
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s
    command: >
      vllm serve openai/gpt-oss-20b
        --host 0.0.0.0
        --port 8000
        --served-model-name
        gpt-oss-20b-32768
        --quantization mxfp4
        --mxfp4-backend CUTLASS
        --mxfp4-layers moe,qkv,o,lm_head
        --attention-backend FLASHINFER
        --kv-cache-dtype fp8
        --tensor-parallel-size 1
        --enable-prefix-caching
        --load-format fastsafetensors
        --max-model-len 32768
        --max-num-seqs 256
        --max-num-batched-tokens 32768
        --gpu_memory_utilization 0.7

  gpt-oss-20b-131072:
    container_name: gpt-oss-20b-131072
    environment:
      - FLASHINFER_LOGLEVEL=0
      - FLASHINFER_JIT_VERBOSE=0
      - FLASHINFER_NVCC_THREADS=4
      - VLLM_NO_USAGE_STATS=1
      - DO_NOT_TRACK=1
      - TIKTOKEN_ENCODINGS_BASE=/workspace/tiktoken_encodings
    build:
      context: .
      dockerfile: Dockerfile.vllm
      args:
        BUILD_JOBS: 16
    image: vllm-mxfp4-spark:latest
    volumes:
      - ${HF_LOC}:/root/.cache/huggingface
      - ./.cache/flashinfer:/root/.cache/flashinfer
      - ./.cache/vllm:/root/.cache/vllm
      - ./.cache/ccache:/root/.ccache
    restart: no
    networks:
      - llama-net
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all
            capabilities:
            - gpu
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    healthcheck:
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8000/health
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s
    command: >
      vllm serve openai/gpt-oss-20b
        --host 0.0.0.0
        --port 8000
        --served-model-name
        gpt-oss-20b-131072
        --quantization mxfp4
        --mxfp4-backend CUTLASS
        --mxfp4-layers moe,qkv,o,lm_head
        --attention-backend FLASHINFER
        --kv-cache-dtype fp8
        --tensor-parallel-size 1
        --enable-prefix-caching
        --load-format fastsafetensors
        --max-model-len 131072
        --max-num-seqs 32
        --max-num-batched-tokens 32768
        --gpu_memory_utilization 0.1

  gpt-oss-120b-131072:
    container_name: gpt-oss-120b-131072
    environment:
      - FLASHINFER_LOGLEVEL=0
      - FLASHINFER_JIT_VERBOSE=0
      - FLASHINFER_NVCC_THREADS=4
      - VLLM_NO_USAGE_STATS=1
      - DO_NOT_TRACK=1
      - TIKTOKEN_ENCODINGS_BASE=/workspace/tiktoken_encodings
    build:
      context: .
      dockerfile: Dockerfile.vllm
      args:
        BUILD_JOBS: 16
    image: vllm-mxfp4-spark:latest
    volumes:
      - ${HF_LOC}:/root/.cache/huggingface
      - ./.cache/flashinfer:/root/.cache/flashinfer
      - ./.cache/vllm:/root/.cache/vllm
      - ./.cache/ccache:/root/.ccache
    restart: no
    networks:
      - llama-net
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all
            capabilities:
            - gpu
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    healthcheck:
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8000/health
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s
    command: >
      vllm serve openai/gpt-oss-120b
        --host 0.0.0.0
        --port 8000
        --served-model-name
        gpt-oss-120b-131072
        --quantization mxfp4
        --mxfp4-backend CUTLASS
        --mxfp4-layers moe,qkv,o,lm_head
        --attention-backend FLASHINFER
        --kv-cache-dtype fp8
        --tensor-parallel-size 1
        --enable-prefix-caching
        --load-format fastsafetensors
        --max-model-len 131072
        --max-num-seqs 3
        --max-num-batched-tokens 65536
        --gpu_memory_utilization 0.7
        --tool-call-parser openai
        --enable-auto-tool-choice

  nemotron-3:
    container_name: nemotron-3
    environment:
      - FLASHINFER_LOGLEVEL=0
      - FLASHINFER_JIT_VERBOSE=0
      - FLASHINFER_NVCC_THREADS=4
      - VLLM_NO_USAGE_STATS=1
      - DO_NOT_TRACK=1
      - TIKTOKEN_ENCODINGS_BASE=/workspace/tiktoken_encodings
      - VLLM_USE_FLASHINFER_MOE_FP4=1
      - VLLM_FLASHINFER_MOE_BACKEND=throughput
    image: scitrera/dgx-spark-vllm:0.14.1-t4
    volumes:
      - ${HF_LOC}:/root/.cache/huggingface
      - ./.cache/flashinfer:/root/.cache/flashinfer
      - ./.cache/vllm:/root/.cache/vllm
      - ./.cache/ccache:/root/.ccache
    restart: no
    networks:
      - llama-net
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all
            capabilities:
            - gpu
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    healthcheck:
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8000/health
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s
    command: >
      vllm serve nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4
        --host 0.0.0.0
        --port 8000
        --served-model-name
        nemotron-3
        --kv-cache-dtype fp8
        --tensor-parallel-size 1
        --enable-prefix-caching
        --load-format fastsafetensors
        --gpu_memory_utilization 0.8
        --trust_remote_code

  glm-4.7-flash:
    container_name: glm-4.7-flash
    environment:
      - FLASHINFER_LOGLEVEL=0
      - FLASHINFER_JIT_VERBOSE=0
      - FLASHINFER_NVCC_THREADS=4
      - VLLM_NO_USAGE_STATS=1
      - DO_NOT_TRACK=1
      - TIKTOKEN_ENCODINGS_BASE=/workspace/tiktoken_encodings
    image: scitrera/dgx-spark-vllm:0.16.0-t5
    volumes:
      - ${HF_LOC}:/root/.cache/huggingface
      - ./.cache/flashinfer:/root/.cache/flashinfer
      - ./.cache/vllm:/root/.cache/vllm
      - ./.cache/ccache:/root/.ccache
    restart: no
    networks:
      - llama-net
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all
            capabilities:
            - gpu
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    healthcheck:
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8000/health
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s
    command: >
      vllm serve zai-org/GLM-4.7-Flash
        --host 0.0.0.0
        --port 8000
        --served-model-name
        glm-4.7-flash
        --tensor-parallel-size 1
        --enable-prefix-caching
        --load-format fastsafetensors
        --gpu_memory_utilization 0.8
        --tool-call-parser glm47
        --reasoning-parser glm45
        --enable-auto-tool-choice
        --max-num-batched-tokens 4096
        --max-model-len 202752

  step-3.5-flash-int4:
    container_name: step-3.5-flash-int4
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - GGML_CUDA_ENABLE_UNIFIED_MEMORY=1
      - LLAMA_ARG_MMAP=0
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_ARG_WEBUI=0
      - LLAMA_ARG_PORT=8000
      - LLAMA_ARG_THREADS=8
      - LLAMA_ARG_FLASH_ATTN=on
      - LLAMA_ARG_N_GPU_LAYERS=all
      - LLAMA_ARG_CTX_SIZE=131072
      - LLAMA_ARG_BATCH=66536
      - LLAMA_ARG_UBATCH=66536
    build:
      dockerfile: Dockerfile.llamacppstep35
      context: .
    image: llama-cpp-step35:arm64cuda
    volumes:
      - ${HF_LOC}:/root/.cache/huggingface
      - ${LLAMACPP_LOC}:/root/.cache/llama.cpp
    restart: no
    networks:
      - llama-net
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all
            capabilities:
            - gpu
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    healthcheck:
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8000/health
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s
    command: ["--metrics", "-m", "/root/.cache/huggingface/hub/models--stepfun-ai--Step-3.5-Flash-Int4/snapshots/8f4ece312b61a4e73f99291a949bdf58dbb7ad1f/step3p5_flash_Q4_K_S.gguf","--temp","1.0"]

  qwen3-coder-next-fp8:
    container_name: qwen3-coder-next-fp8
    environment:
      - FLASHINFER_LOGLEVEL=0
      - FLASHINFER_JIT_VERBOSE=0
      - FLASHINFER_NVCC_THREADS=4
      - VLLM_NO_USAGE_STATS=1
      - DO_NOT_TRACK=1
      - TIKTOKEN_ENCODINGS_BASE=/workspace/tiktoken_encodings
    image: scitrera/dgx-spark-vllm:0.16.0-t5
    volumes:
      - ${HF_LOC}:/root/.cache/huggingface
      - ./.cache/flashinfer:/root/.cache/flashinfer
      - ./.cache/vllm:/root/.cache/vllm
      - ./.cache/ccache:/root/.ccache
    restart: no
    networks:
      - llama-net
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all
            capabilities:
            - gpu
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    healthcheck:
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8000/health
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s
    command: >
      vllm serve Qwen/Qwen3-Coder-Next-FP8
        --host 0.0.0.0
        --port 8000
        --served-model-name
        qwen3-coder-next-fp8
        --tensor-parallel-size 1
        --enable-prefix-caching
        --load-format fastsafetensors
        --gpu_memory_utilization 0.8
        --tool-call-parser qwen3_coder
        --enable-auto-tool-choice
        --max-num-batched-tokens 4096
        --max-model-len 202752

  qwen3-coder-next-mxfp4:
    container_name: qwen3-coder-next-mxfp4
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - GGML_CUDA_ENABLE_UNIFIED_MEMORY=1
      - LLAMA_ARG_MMAP=0
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_ARG_WEBUI=0
      - LLAMA_ARG_PORT=8000
      - LLAMA_ARG_THREADS=20
      - LLAMA_ARG_FLASH_ATTN=on
      - LLAMA_ARG_N_GPU_LAYERS=all
      - LLAMA_ARG_BATCH=4096
      - LLAMA_ARG_UBATCH=4096
    build:
      dockerfile: Dockerfile.llamacpp
      context: .
    image: llama-cpp:arm64cuda
    volumes:
      - ${HF_LOC}:/root/.cache/huggingface
      - ${LLAMACPP_LOC}:/root/.cache/llama.cpp
    restart: no
    networks:
      - llama-net
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all
            capabilities:
            - gpu
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    healthcheck:
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8000/health
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s
    command: ["--metrics", "-m", "/root/.cache/huggingface/hub/models--unsloth--Qwen3-Coder-Next-GGUF/snapshots/37d32cdfb01580f72e8f73f35f8c18e7e40b28ae/Qwen3-Coder-Next-MXFP4_MOE.gguf","--temp","1.0"]

  minimax-m2.5-gguf-q3:
    container_name: minimax-m2.5-gguf-q3
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - GGML_CUDA_ENABLE_UNIFIED_MEMORY=1
      - LLAMA_ARG_MMAP=0
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_ARG_WEBUI=0
      - LLAMA_ARG_PORT=8000
      - LLAMA_ARG_THREADS=20
      - LLAMA_ARG_FLASH_ATTN=on
      - LLAMA_ARG_N_GPU_LAYERS=all
    build:
      dockerfile: Dockerfile.llamacpp
      context: .
    image: llama-cpp:arm64cuda
    volumes:
      - ${HF_LOC}:/root/.cache/huggingface
      - ${LLAMACPP_LOC}:/root/.cache/llama.cpp
    restart: no
    networks:
      - llama-net
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all
            capabilities:
            - gpu
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    healthcheck:
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8000/health
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s
    command: ["--metrics", "-m", "/root/.cache/huggingface/hub/models--ox-ox--MiniMax-M2.5-GGUF/snapshots/076c3e199a449df2ec01b6d52a762ee56fb9d702/minimax-m2.5-Q3_K_L.gguf"]

  nemotron-3-gguf-bf16:
    container_name: nemotron-3-gguf-bf16
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - GGML_CUDA_ENABLE_UNIFIED_MEMORY=1
      - LLAMA_ARG_MMAP=0
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_ARG_WEBUI=0
      - LLAMA_ARG_PORT=8000
      - LLAMA_ARG_THREADS=20
      - LLAMA_ARG_FLASH_ATTN=on
      - LLAMA_ARG_N_GPU_LAYERS=all
      - LLAMA_ARG_BATCH=4096
      - LLAMA_ARG_UBATCH=4096
      - HF_TOKEN=${HF_TOKEN}
    build:
      dockerfile: Dockerfile.llamacpp
      context: .
    image: llama-cpp:arm64cuda
    volumes:
      - ${HF_LOC}:/root/.cache/huggingface
      - ${LLAMACPP_LOC}:/root/.cache/llama.cpp
    restart: no
    networks:
      - llama-net
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all
            capabilities:
            - gpu
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    healthcheck:
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8000/health
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s
    command: ["--metrics", "-hf", "unsloth/Nemotron-3-Nano-30B-A3B-GGUF:BF16","--temp","1.0"]

  glm-4.6v-flash-gguf-bf16:
    container_name: glm-4.6v-flash-gguf-bf16
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - GGML_CUDA_ENABLE_UNIFIED_MEMORY=1
      - LLAMA_ARG_MMAP=0
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_ARG_WEBUI=0
      - LLAMA_ARG_PORT=8000
      - LLAMA_ARG_THREADS=20
      - LLAMA_ARG_FLASH_ATTN=on
      - LLAMA_ARG_N_GPU_LAYERS=all
      - LLAMA_ARG_BATCH=4096
      - LLAMA_ARG_UBATCH=4096
    build:
      dockerfile: Dockerfile.llamacpp
      context: .
    image: llama-cpp:arm64cuda
    volumes:
      - ${HF_LOC}:/root/.cache/huggingface
      - ${LLAMACPP_LOC}:/root/.cache/llama.cpp
    restart: no
    networks:
      - llama-net
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all
            capabilities:
            - gpu
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    healthcheck:
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8000/health
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s
    command: ["--metrics", "-hf", "unsloth/GLM-4.6V-Flash-GGUF:BF16","--temp","0.8"]


networks:
  llama-net:
    external: true
    name: llama-net


configs:
  qdrant_config:
    content: |
      log_level: INFO
