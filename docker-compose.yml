services:

  loki:
    image: grafana/loki:latest
    container_name: loki
    restart: unless-stopped
    networks:
      - llama-net
    command: ["-config.file=/etc/loki/loki-config.yml"]
    ports:
      - "8008:3100"
    volumes:
      - ./loki_data/wal:/wal:rw
      - ./loki_data/data:/loki:rw
      - ./config/loki.yml:/etc/loki/loki-config.yml:ro

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    networks:
        - llama-net
    ports:
      - "8010:3000"

    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=SuperSecret123
      - GF_SERVER_ROOT_URL=http://localhost:3000

    volumes:
      - ./grafana-data:/var/lib/grafana:rw

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s

  prometheus:
    image: prom/prometheus:latest          # official Prometheus image
    container_name: prometheus
    restart: unless-stopped               # keep the container alive
    networks:
        - llama-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9090/api/v1/query?query=up"]
      interval: 30s                     # run every 30â€¯s
      timeout: 10s
      retries: 5
      start_period: 5s                  # grace period before first test
    ports:
      - "8009:9090"                     # Grafana UI & Prometheus UI (both reachable)
    volumes:
      - ./config/prometheus.yml:/prometheus/prometheus.yml:ro   # Prometheus config
      - ./prometheus_data:/prometheus:rw   # Prometheus DB, alert rules, etc.
  nvidia-smi-exporter:
    image: utkuozdemir/nvidia_gpu_exporter:1.3.1
    container_name: nvidia-smi-exporter
    restart: unless-stopped
    networks:
        - llama-net
    volumes:
      - /usr/lib/aarch64-linux-gnu/libnvidia-ml.so:/usr/lib/aarch64-linux-gnu/libnvidia-ml.so
      - /usr/lib/aarch64-linux-gnu/libnvidia-ml.so.1:/usr/lib/aarch64-linux-gnu/libnvidia-ml.so.1
      - /usr/bin/nvidia-smi:/usr/bin/nvidia-smi 
    devices:
      - /dev/nvidia0:/dev/nvidia0
      - /dev/nvidiactl:/dev/nvidiactl

  llama-swap:
    image: local-llama-swap:latest
    restart: always
    container_name: llama-swap
    build:
      context: .
      dockerfile: Dockerfile.llamaswap
    networks:
      - llama-net
    ports:
      - 8000:8080
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./config/llama-swap.yaml:/app/config.yaml:ro
      - ./docker-compose.models.yml:/app/docker-compose.models.yml:ro
      - ./docker-compose.models.env:/app/docker-compose.models.env:ro
networks:
  llama-net:
    name: llama-net
    driver: bridge
