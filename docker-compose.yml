# =============================================================================
# Docker Compose for GPT-OSS-120B with MXFP4 on DGX Spark (SM121/GB10)
# =============================================================================
#
# Achieves 59.4 tok/s decode with CUTLASS MXFP4 kernel.
#
# Usage:
#   docker compose build                 # Build the image
#   docker compose up -d                 # Start the service
#   docker compose logs -f               # View logs
#   docker compose down                  # Stop the service
#
# Benchmark:
#   docker compose exec vllm-mxfp4 llama-benchy \
#       --base-url http://localhost:8000/v1 \
#       --model gpt-oss-120b \
#       --tokenizer openai/gpt-oss-120b \
#       --pp 2048 --tg 32 128
#
# =============================================================================

services:

  loki:
    image: grafana/loki:latest
    container_name: loki
    restart: unless-stopped
    networks:
      - llama-net
    command: ["-config.file=/etc/loki/loki-config.yml"]
    ports:
      - "8008:3100"
    volumes:
      - ./loki_data/wal:/wal:rw
      - ./loki_data/data:/loki:rw
      - ./config/loki.yml:/etc/loki/loki-config.yml:ro

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    networks:
        - llama-net
    ports:
      - "8010:3000"

    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=SuperSecret123
      - GF_SERVER_ROOT_URL=http://localhost:3000

    volumes:
      - ./grafana-data:/var/lib/grafana:rw

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s

  prometheus:
    image: prom/prometheus:latest          # official Prometheus image
    container_name: prometheus
    restart: unless-stopped               # keep the container alive
    networks:
        - llama-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9090/api/v1/query?query=up"]
      interval: 30s                     # run every 30â€¯s
      timeout: 10s
      retries: 5
      start_period: 5s                  # grace period before first test
    ports:
      - "8009:9090"                     # Grafana UI & Prometheus UI (both reachable)
    volumes:
      - ./config/prometheus.yml:/prometheus/prometheus.yml:ro   # Prometheus config
      - ./prometheus_data:/prometheus:rw   # Prometheus DB, alert rules, etc.
  nvidia-smi-exporter:
    image: utkuozdemir/nvidia_gpu_exporter:1.3.1
    container_name: nvidia-smi-exporter
    restart: unless-stopped
    networks:
        - llama-net
    volumes:
      - /usr/lib/aarch64-linux-gnu/libnvidia-ml.so:/usr/lib/aarch64-linux-gnu/libnvidia-ml.so
      - /usr/lib/aarch64-linux-gnu/libnvidia-ml.so.1:/usr/lib/aarch64-linux-gnu/libnvidia-ml.so.1
      - /usr/bin/nvidia-smi:/usr/bin/nvidia-smi 
    devices:
      - /dev/nvidia0:/dev/nvidia0
      - /dev/nvidiactl:/dev/nvidiactl

  postgres:
    image: postgres:16-alpine
    restart: unless-stopped
    networks:
        - llama-net
    environment:
      POSTGRES_USER: n8n
      POSTGRES_PASSWORD: n8n
      POSTGRES_DB: n8n
    volumes:
      - ./postgres_data:/var/lib/postgresql/data

  n8n:
    image: n8nio/n8n:latest
    restart: unless-stopped
    networks:
        - llama-net
    depends_on:
      - postgres
    ports:
      - "5678:5678"
    environment:
      DB_TYPE: postgresdb
      DB_POSTGRESDB_HOST: postgres
      DB_POSTGRESDB_PORT: "5432"
      DB_POSTGRESDB_DATABASE: n8n
      DB_POSTGRESDB_USER: n8n
      DB_POSTGRESDB_PASSWORD: n8n
      N8N_PORT: "5678"
      N8N_PROTOCOL: http
      N8N_HOST: 127.0.0.1
      N8N_SECURE_COOKIE: false
      N8N_RESTRICT_FILE_ACCESS_TO: /home/node/n8n_files
      WEBHOOK_URL: http://127.0.0.1:5678/
      GENERIC_TIMEZONE: UTC
      NODE_OPTIONS: "--max-old-space-size=4096"

      TZ: UTC
    volumes:
      - ./n8n_data:/home/node/.n8n
      - ./n8n_files:/home/node/n8n_files

  qdrant:
    image: qdrant/qdrant:latest
    restart: always
    container_name: qdrant
    networks:
      - llama-net
    environment:
      QDRANT__GPU__INDEXING: 1
    ports:
      - 6333:6333
      - 6334:6334
    configs:
      - source: qdrant_config
        target: /qdrant/config/production.yaml
    volumes:
      - ./qdrant_data:/qdrant/storage

  llama-swap:
    image: local-llama-swap:latest
    restart: always
    container_name: llama-swap
    build:
      context: .
      dockerfile: Dockerfile.llamaswap
    networks:
      - llama-net
    ports:
      - 8000:8080
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./config/llama-swap.yaml:/app/config.yaml:ro
      - ./docker-compose.models.yml:/app/docker-compose.models.yml:ro

configs:
  qdrant_config:
    content: |
      log_level: INFO

networks:
  llama-net:
    name: llama-net
    driver: bridge
